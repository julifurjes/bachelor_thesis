---
title: "bachelor"
author: "Juli Furjes"
date: "2022-11-08"
output: html_document
---

## LOADING DATA AND PACKAGES

```{r}
library(ggplot2)
library(tidyverse)
library(scales)
library(lme4)
require(gridExtra)
library(ggpubr)
library(car)
library(RColorBrewer)     

data <- read.csv('data.csv', sep=';')
conditions <- read.csv('conditions.csv', sep=';')

# removing NAs from data
data <- na.omit(data)
```

## FLEXIBILITY

### creating datasets

```{r}
# calculating how many unique categories are there per subject

# fork
data_flex_fork <- aggregate(data = data,
                          fork_category ~ subject,
                          na.action = na.omit,
                          function(x) length(unique(x)))

colnames(data_flex_fork) <- c("subject", "fork_category")

data_flex_fork

# shoe
data_flex_shoe <- aggregate(data = data,
                          shoe_category ~ subject,
                          na.action = na.omit,
                          function(x) length(unique(x)))

colnames(data_flex_shoe) <- c("subject", "shoe_category")

data_flex_shoe

data_flex <- tibble(data_flex_fork, data_flex_shoe$shoe_category, conditions$condition)
colnames(data_flex) <- c("subject", "fork_category", "shoe_category", "condition")

# adding a column with the difference
data_flex$diff <- data_flex$shoe_category - data_flex$fork_category

data_flex
```

### analysis

```{r}
data_flex
```

#### plots

```{r}
theme_set(theme_bw())

violin_categories <- ggplot(data_flex, aes(fork_category, shoe_category))
violin_categories + geom_violin() + 
  labs(title="Violin plot", 
       subtitle="City Mileage vs Class of vehicle",
       caption="Source: mpg",
       x="Class of Vehicle",
       y="City Mileage") +
  geom_violin(trim=FALSE)

# jitter plot
# fork
fork_jitter_flex <- ggplot(data_flex, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = fork_category, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Flexibility based on conditions - fork", 
       subtitle = "Number of different categories per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# shoe
shoe_jitter_flex <- ggplot(data_flex, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = shoe_category, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Flexibility based on conditions - shoe", 
       subtitle = "Number of different categories per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# TODO: merge them in a way that the scale is identical
# merging them
grid.arrange(fork_jitter_flex, shoe_jitter_flex, ncol=1)
```

## ORIGINALITY

### creating datasets

#### calculating the points per categories

```{r}
# finding the unique values and their occurrences in the categories

# fork
fork_categories <- table(data$fork_category)
fork_categories

# shoe
shoe_categories <- table(data$shoe_category)
shoe_categories

# converting them to a dataframe

# fork
fork_categories <- data.frame(fork_categories)
colnames(fork_categories) <- c("fork_category", "occurrence")
fork_categories

# shoe
shoe_categories <- data.frame(shoe_categories)
colnames(shoe_categories) <- c("shoe_category", "occurrence")
shoe_categories

# visualizing
# subsetting categories which appear at least 3 times
fork_main_categories <- fork_categories[fork_categories$occurrence>2, ]
shoe_main_categories <- shoe_categories[shoe_categories$occurrence>2, ]

# extending the colour palette
mycolors_fork <- colorRampPalette(brewer.pal(8, "Greens"))(17)
mycolors_shoe <- colorRampPalette(brewer.pal(8, "Greens"))(16)

ggplot(data=fork_main_categories, aes(x=reorder(fork_category, -occurrence), y=occurrence, fill=fork_category)) +
  geom_bar(position="dodge",stat="identity", fill="darkgreen") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.line = element_line(color='black'),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_blank()) +
  labs(y = "Occurrence", x = "Category")

ggplot(data=shoe_main_categories, aes(x=reorder(shoe_category, -occurrence), y=occurrence, fill=fork_category)) +
  geom_bar(position="dodge",stat="identity", fill="darkgreen") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.line = element_line(color='black'),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_blank()) +
  labs(y = "Occurrence", x = "Category")


# calculating the total number of entries (without na values)

# fork
fork_categories_all <- is.na(data$fork_category)
fork_categories_all <- length(fork_categories_all[fork_categories_all== FALSE])
fork_categories_all

# shoe
shoe_categories_all <- is.na(data$shoe_category)
shoe_categories_all <- length(shoe_categories_all[shoe_categories_all== FALSE])
shoe_categories_all

# calculating frequency percentage for each category

# fork
fork_categories$frequency <- fork_categories$occurrence/fork_categories_all

# shoe
shoe_categories$frequency <- shoe_categories$occurrence/shoe_categories_all

# calculating the points
# less than 0.01: 2 points
# between 0.01 and 0.05: 1 point

# fork
fork_categories$point <- 0

for (i in 1:length(fork_categories$frequency)) {
  fork_categories$point[i][fork_categories$frequency[i] <= as.double(0.01)] <- 2
  fork_categories$point[i][fork_categories$frequency[i] > as.double(0.01) & fork_categories$frequency[i] <= as.double(0.05)] <- 1
}

fork_categories

# shoe
shoe_categories$point <- 0

for (i in 1:length(shoe_categories$frequency)) {
  shoe_categories$point[i][shoe_categories$frequency[i] <= as.double(0.01)] <- 2
  shoe_categories$point[i][shoe_categories$frequency[i] > as.double(0.01) & shoe_categories$frequency[i] <= as.double(0.05)] <- 1
}

shoe_categories

# calculating the points (in a new system)
# less than 0.01: 4 points
# between 0.01 and 0.02: 3 points
# between 0.02 and 0.06: 2 points
# between 0.06 and 0.15: 1 point

# Cropley, A. J. (1967). Creativity, intelligence, and achievement. Alberta Journal of Educational Research, 13, 51â€“58.

# fork
# fork_categories$point <- 0
# 
# for (i in 1:length(fork_categories$frequency)) {
#   fork_categories$point[i][fork_categories$frequency[i] <= as.double(0.01)] <- 4
#   fork_categories$point[i][fork_categories$frequency[i] > as.double(0.01) & fork_categories$frequency[i] <= as.double(0.02)] <- 3
#   fork_categories$point[i][fork_categories$frequency[i] > as.double(0.02) & fork_categories$frequency[i] <= as.double(0.06)] <- 2
#   fork_categories$point[i][fork_categories$frequency[i] > as.double(0.06) & fork_categories$frequency[i] <= as.double(0.15)] <- 1
# }
# 
# fork_categories
# 
# # shoe
# shoe_categories$point <- 0
# 
# for (i in 1:length(shoe_categories$frequency)) {
#   shoe_categories$point[i][shoe_categories$frequency[i] <= as.double(0.01)] <- 4
#   shoe_categories$point[i][shoe_categories$frequency[i] > as.double(0.01) & shoe_categories$frequency[i] <= as.double(0.02)] <- 3
#   shoe_categories$point[i][shoe_categories$frequency[i] > as.double(0.02) & shoe_categories$frequency[i] <= as.double(0.06)] <- 2
#   shoe_categories$point[i][shoe_categories$frequency[i] > as.double(0.06) & shoe_categories$frequency[i] <= as.double(0.15)] <- 1
# }
# 
# shoe_categories
```

#### assigning the points to subjects and conditions

```{r}
# fork
data_ori_fork <- data %>%
  select(subject, fork_category)
data_ori_fork

merged_ori_fork <- merge(data_ori_fork, fork_categories, by = "fork_category", sort = F, all.x = T)
merged_ori_fork

# calculating the originality point per subjects
ori_points_fork <- merged_ori_fork %>%
  na.omit() %>%
  group_by(subject) %>%
  summarise(fork_points = sum(point))

# calculating the average originality occurrence
ori_perc_fork <- 

# shoe
data_ori_shoe <- data %>%
  select(subject, shoe_category)
data_ori_shoe

merged_ori_shoe <- merge(data_ori_shoe, shoe_categories, by = "shoe_category", sort = F, all.x = T)
merged_ori_shoe

# calculating the originality point per subjects
ori_points_shoe <- merged_ori_shoe %>%
  na.omit() %>%
  group_by(subject) %>%
  summarise(shoe_points = sum(point))

# merging the two conditions
data_ori <- tibble(ori_points_fork, ori_points_shoe$shoe_points, conditions$condition)
colnames(data_ori) <- c("subject", "fork_points", "shoe_points", "condition")

# adding a column with the difference
data_ori$diff <- data_ori$shoe_points - data_ori$fork_points

data_ori
```

### analysis

```{r}
data_ori
```

### plots

## FLUENCY

### creating datasets

```{r}
# calculating how many unique values are there per subject

# fork
data_flu_fork <- aggregate(data = data,
                          fork_use ~ subject,
                          na.action = na.omit,
                          function(x) length(unique(x)))

colnames(data_flu_fork) <- c("subject", "fork_use")

data_flu_fork

# shoe
data_flu_shoe <- aggregate(data = data,
                          shoe_use ~ subject,
                          na.action = na.omit,
                          function(x) length(unique(x)))

colnames(data_flu_shoe) <- c("subject", "shoe_use")

data_flu_shoe

data_flu <- tibble(data_flu_fork, data_flu_shoe$shoe_use, conditions$condition, conditions$creativity, conditions$task_success)
colnames(data_flu) <- c("subject", "fork_use", "shoe_use", "condition", "creativity", "task_success")

# adding a column with the difference
data_flu$diff <- data_flu$shoe_use - data_flu$fork_use

data_flu

mean(data_flu$shoe_use)
sd(data_flu$shoe_use)
```

### analysis

```{r}
data_flu

# TODO: check whether creativity predicts difference
# TODO: check fork and shoe separately with creativity as a random effect

# linear regression
flu_model1 <- summary(glm(condition ~ diff, family = binomial(link = logit), data = data_flu))

# mixed effects models
flu_model2 <- lme4::glmer(condition ~ diff + (1 | creativity), family = binomial(link = logit), data_flu)
flu_model3 <- lme4::glmer(condition ~ diff + (1 | task_success), family = binomial(link = logit), data_flu)
# if singular fit: the random effect doesn't have a big enough effect, so removing it is advised

coef(flu_model1)$creativity
```

### plots

```{r}
# box plot
ggplot(data_flu, 
       aes(x = as.factor(condition), 
           y = fork_use)) +
  geom_boxplot() +
  labs(title = "Fluency across conditions",
       x = "Condition",
       y = "Fluency") +
  scale_x_discrete(labels=c("0" = "Non-playful", "1" = "Playful"))

# jitter plot
# fork
fork_jitter_flu <- ggplot(data_flu, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = fork_use, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Fluency based on conditions - fork", 
       subtitle = "Number of ideas per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# shoe
shoe_jitter_flu <- ggplot(data_flu, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = shoe_use, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Fluency based on conditions - shoe", 
       subtitle = "Number of ideas per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# TODO: merge them in a way that the scale is identical
# merging them
grid.arrange(fork_jitter_flu, shoe_jitter_flu, ncol=1)
```

## ELABORATION

### creating datasets

```{r}
# calculating the mean per subject
# (the values were calculated calculated manually in the datasheet)

# fork
data_ela_fork <- aggregate(data = data,
                          as.numeric(fork_elaboration) ~ subject,
                          na.action = na.omit,
                          mean)

colnames(data_ela_fork) <- c("subject", "fork_word_count")

# rounding numbers
data_ela_fork$fork_word_count <- round(data_ela_fork$fork_word_count, digits=2)

data_ela_fork

# shoe
data_ela_shoe <- aggregate(data = data,
                          as.numeric(shoe_elaboration) ~ subject,
                          na.action = na.omit,
                          mean)

colnames(data_ela_shoe) <- c("subject", "shoe_word_count")

# rounding numbers
data_ela_shoe$shoe_word_count <- round(data_ela_shoe$shoe_word_count, digits=2)

data_ela_shoe

data_ela <- tibble(data_ela_fork, data_ela_shoe$shoe_word_count, conditions$condition)
colnames(data_ela) <- c("subject", "fork_word_count", "shoe_word_count", "condition")

# adding a column with the difference
data_ela$diff <- data_ela$shoe_word_count - data_ela$fork_word_count

data_ela
```

### analysis

```{r}
data_ela
```

### plots

```{r}
# jitter plot
# fork
fork_jitter_ela <- ggplot(data_ela, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = fork_word_count, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Elaboration based on conditions - fork", 
       subtitle = "Number of meaningful words per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# shoe
shoe_jitter_ela <- ggplot(data_ela, 
       aes(y = factor(condition,
                      labels = c("Non-playful",
                                 "Playful")), 
           x = shoe_word_count, 
           color = condition)) +
  geom_jitter(alpha = 0.7,
              size = 1.5) + 
  scale_x_continuous() +
  labs(title = "Elaboration based on conditions - shoe", 
       subtitle = "Number of meaningful words per participant",
       x = "",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

# TODO: merge them in a way that the scale is identical
# merging them
grid.arrange(fork_jitter_ela, shoe_jitter_ela, ncol=1)
```

## IDEAS

- One-way ANCOVAs were carried out on each average AUT score with condition as the between subjects factor (impro- visation and control). Pre-test scores were taken as the covariate. Preliminary checks revealed no violation of assumptions. (Lewis and Lovatt)

- Word cloud with the categories?

- Check whether originality is related to fluency

- MANOVA on condition + success + initial creativity?
Gender x high/low playfulness (2 x 2) MANOVAs were conducted separately, for the self- and other-rated descriptors.
https://www.researchoptimus.com/article/what-is-manova.php
ANOVA uses the F-test to determine whether the variability between group means is larger than the variability of the observations within the groups. If that ratio is sufficiently large, you can conclude that not all the means are equal.

- For self-ratings, participants who rated themselves as â€˜4â€™ or lower (bottom 12%) on their playfulness rating were assigned to the low playfulness group; those who rated themselves as â€˜9â€™ or above (upper 15%) were placed in the high playfulness group.

- Check whether the ones who just rebuilt the prototype have correlation with the creativity outcome

## CORRELATIONS

### creating a merged table

```{r}
data_merged <- tibble(data_flex$subject, data_flex$condition, conditions$creativity, data_flex$fork_category, data_flex$shoe_category, data_ori$fork_points, data_ori$shoe_points, data_flu$fork_use, data_flu$shoe_use, data_ela$fork_word_count, data_ela$shoe_word_count)
colnames(data_merged) <- c("subject", "condition", "creativity", "fork_flex", "shoe_flex", "fork_ori", "shoe_ori", "fork_flu", "shoe_flu", "fork_ela", "shoe_ela")

# adding an extra column for the sum of points
data_merged$fork_all <- (data_merged$fork_ela + data_merged$fork_flex + data_merged$fork_flu + data_merged$fork_ori)
data_merged$shoe_all <- (data_merged$shoe_ela + data_merged$shoe_flex + data_merged$shoe_flu + data_merged$shoe_ori)
data_merged$diff_all <- (data_merged$shoe_all - data_merged$fork_all)

# creating a dataset for each condition
data_merged_np <- data_merged %>% 
  filter(condition == 0)

data_merged_p <- data_merged %>% 
  filter(condition == 1)

data_merged
data_merged_np
data_merged_p
```

### overall points and condition

```{r}
anova_model <- aov(diff_all ~ condition + creativity, data = data_merged)
summary(anova_model)

# non-playful
t.test(data_merged_np$fork_all, data_merged_np$shoe_all, paired = TRUE)

# playful
t.test(data_merged_p$fork_all, data_merged_p$shoe_all, paired = TRUE)
```

### originality and fluency

```{r}
# fork
ancova_model <- aov(fork_ori ~ fork_flu, data = data_merged)
summary(ancova_model)

flu_ori_fork <- lm(fork_ori ~ fork_flu, data=data_merged)
summary(flu_ori_fork)
summary(flu_ori_fork)$r.squared

# creating the plot
plot(data_merged$fork_ori, data_merged$fork_flu, pch = 19, col = "lightblue")

# regression line
abline(lm(data_merged$fork_ori ~ data_merged$fork_flex), col = "red", lwd = 3)

# Pearson correlation
text(paste("Correlation:", round(cor(data_merged$fork_ori, data_merged$fork_flu), 2), "  R squared: ", round(summary(flu_ori_shoe)$r.squared, 2)), x = 2.5, y = 12)

# shoe
ancova_model <- aov(shoe_ori ~ shoe_flu, data = data_merged)
summary(ancova_model)

flu_ori_shoe <- lm(shoe_ori ~ shoe_flu, data=data_merged)
summary(flu_ori_shoe)
summary(flu_ori_shoe)$r.squared

# creating the plot
plot(data_merged$shoe_ori, data_merged$shoe_flu, pch = 19, col = "lightblue")

# regression line
abline(lm(data_merged$shoe_ori ~ data_merged$shoe_flex), col = "red", lwd = 3)

# Pearson correlation
text(paste("Correlation:", round(cor(data_merged$shoe_ori, data_merged$shoe_flu), 2), "  R squared: ", round(summary(flu_ori_fork)$r.squared, 2)), x = 2.5, y = 12)

boxplot(exam ~ technique,
data = data_merged,
main = "Score by Technique",
xlab = "Technique",ylab = "Score",
col = "red",border = "black")
```

### flexibility and fluency

```{r}
# creating the plot
plot(data_merged$fork_flex, data_merged$fork_flu, pch = 19, col = "lightblue")

# regression line
abline(lm(data_merged$fork_flu ~ data_merged$fork_flex), col = "red", lwd = 3)

# Pearson correlation
text(paste("Correlation:", round(cor(data_merged$fork_flex, data_merged$fork_flu), 2)), x = 2.5, y = 12)

boxplot(exam ~ technique,
data = data_merged,
main = "Score by Technique",
xlab = "Technique",ylab = "Score",
col = "red",border = "black")
```

## OTHER

### summarising table and plot

```{r}
# non-playful
data_merged_np <- data_merged %>% filter(condition == 0)
data_merged_np

# create matrix with 4 columns and 2 rows
sum_table_np = matrix(c(round(mean(data_merged_np$fork_flex), 2), round(mean(data_merged_np$fork_ori), 2), round(mean(data_merged_np$fork_flu), 2), round(mean(data_merged_np$fork_ela), 2), round(mean(data_merged_np$shoe_flex), 2), round(mean(data_merged_np$shoe_ori), 2), round(mean(data_merged_np$shoe_flu), 2), round(mean(data_merged_np$shoe_ela), 2)), ncol=4, byrow=TRUE)
 
# specify the column names and row names of matrix
colnames(sum_table_np) = c('Flexibility','Originality','Fluency','Elaboration')
rownames(sum_table_np) <- c('Pre-intervention','Post-intervention')
 
# assign to table
final_sum_table_np=as.table(sum_table_np)

final_sum_table_np

aspect <- rep(c('Flexibility','Originality','Fluency','Elaboration') , 2)
condition <- c(rep("pre-intervention" , 4) , rep("post-intervention" , 4))
value <- c(round(mean(data_merged_np$fork_flex), 2), round(mean(data_merged_np$fork_ori), 2), round(mean(data_merged_np$fork_flu), 2), round(mean(data_merged_np$fork_ela), 2), round(mean(data_merged_np$shoe_flex), 2), round(mean(data_merged_np$shoe_ori), 2), round(mean(data_merged_np$shoe_flu), 2), round(mean(data_merged_np$shoe_ela), 2))
final_sum_table_np_for_plot <- data.frame(aspect,condition,value)

ggplot(final_sum_table_np_for_plot, aes(fill=condition, y=value, x=aspect)) + 
    geom_bar(position="dodge", stat="identity")

# playful
data_merged_p <- data_merged %>% filter(condition == 1)
data_merged_p

# create matrix with 4 columns and 2 rows
sum_table_p = matrix(c(round(mean(data_merged_p$fork_flex), 2), round(mean(data_merged_p$fork_ori), 2), round(mean(data_merged_p$fork_flu), 2), round(mean(data_merged_p$fork_ela), 2), round(mean(data_merged_p$shoe_flex), 2), round(mean(data_merged_p$shoe_ori), 2), round(mean(data_merged_p$shoe_flu), 2), round(mean(data_merged_p$shoe_ela), 2)), ncol=4, byrow=TRUE)
 
# specify the column names and row names of matrix
colnames(sum_table_p) = c('Flexibility','Originality','Fluency','Elaboration')
rownames(sum_table_p) <- c('Pre-intervention','Post-intervention')
 
# assign to table
final_sum_table_p=as.table(sum_table_p)

final_sum_table_p

aspect <- rep(c('Flexibility','Originality','Fluency','Elaboration') , 2)
condition <- c(rep("pre-intervention" , 4) , rep("post-intervention" , 4))
value <- c(round(mean(data_merged_p$fork_flex), 2), round(mean(data_merged_p$fork_ori), 2), round(mean(data_merged_p$fork_flu), 2), round(mean(data_merged_p$fork_ela), 2), round(mean(data_merged_p$shoe_flex), 2), round(mean(data_merged_p$shoe_ori), 2), round(mean(data_merged_p$shoe_flu), 2), round(mean(data_merged_p$shoe_ela), 2))
final_sum_table_p_for_plot <- data.frame(aspect,condition,value)

ggplot(final_sum_table_p_for_plot, aes(fill=condition, y=value, x=aspect)) + 
    geom_bar(position="dodge", stat="identity")
```
### standard deviations

```{r}
# non-playful

# create matrix with 4 columns and 2 rows
sum_table_np_sd = matrix(c(round(sd(data_merged_np$fork_flex), 2), round(sd(data_merged_np$fork_ori), 2), round(sd(data_merged_np$fork_flu), 2), round(sd(data_merged_np$fork_ela), 2), round(sd(data_merged_np$shoe_flex), 2), round(sd(data_merged_np$shoe_ori), 2), round(sd(data_merged_np$shoe_flu), 2), round(sd(data_merged_np$shoe_ela), 2)), ncol=4, byrow=TRUE)
 
# specify the column names and row names of matrix
colnames(sum_table_np_sd) = c('Flexibility','Originality','Fluency','Elaboration')
rownames(sum_table_np_sd) <- c('Pre-intervention','Post-intervention')
 
# assign to table
final_sum_table_np_sd=as.table(sum_table_np_sd)

final_sum_table_np_sd

aspect <- rep(c('Flexibility','Originality','Fluency','Elaboration') , 2)
condition <- c(rep("pre-intervention" , 4) , rep("post-intervention" , 4))
value <- c(round(sd(data_merged_np$fork_flex), 2), round(sd(data_merged_np$fork_ori), 2), round(sd(data_merged_np$fork_flu), 2), round(sd(data_merged_np$fork_ela), 2), round(sd(data_merged_np$shoe_flex), 2), round(sd(data_merged_np$shoe_ori), 2), round(sd(data_merged_np$shoe_flu), 2), round(sd(data_merged_np$shoe_ela), 2))
final_sum_table_np_for_plot <- data.frame(aspect,condition,value)

ggplot(final_sum_table_np_for_plot, aes(fill=condition, y=value, x=aspect)) + 
    geom_bar(position="dodge", stat="identity")

# playful

# create matrix with 4 columns and 2 rows
sum_table_p_sd = matrix(c(round(sd(data_merged_p$fork_flex), 2), round(sd(data_merged_p$fork_ori), 2), round(sd(data_merged_p$fork_flu), 2), round(sd(data_merged_p$fork_ela), 2), round(sd(data_merged_p$shoe_flex), 2), round(sd(data_merged_p$shoe_ori), 2), round(sd(data_merged_p$shoe_flu), 2), round(sd(data_merged_p$shoe_ela), 2)), ncol=4, byrow=TRUE)
 
# specify the column names and row names of matrix
colnames(sum_table_p_sd) = c('Flexibility','Originality','Fluency','Elaboration')
rownames(sum_table_p_sd) <- c('Pre-intervention','Post-intervention')
 
# assign to table
final_sum_table_p_sd=as.table(sum_table_p_sd)

final_sum_table_p_sd

aspect <- rep(c('Flexibility','Originality','Fluency','Elaboration') , 2)
condition <- c(rep("pre-intervention" , 4) , rep("post-intervention" , 4))
value <- c(round(sd(data_merged_p$fork_flex), 2), round(sd(data_merged_p$fork_ori), 2), round(sd(data_merged_p$fork_flu), 2), round(sd(data_merged_p$fork_ela), 2), round(sd(data_merged_p$shoe_flex), 2), round(sd(data_merged_p$shoe_ori), 2), round(sd(data_merged_p$shoe_flu), 2), round(sd(data_merged_p$shoe_ela), 2))
final_sum_table_p_for_plot <- data.frame(aspect,condition,value)
```
### calculating differences between scores

```{r}
# non-playful
mean(data_merged_np$shoe_flex - data_merged_np$fork_flex)
mean(data_merged_np$shoe_ori - data_merged_np$fork_ori)
mean(data_merged_np$shoe_flu - data_merged_np$fork_flu)
mean(data_merged_np$shoe_ela - data_merged_np$fork_ela)

sd(data_merged_np$shoe_flex - data_merged_np$fork_flex)
sd(data_merged_np$shoe_ori - data_merged_np$fork_ori)
sd(data_merged_np$shoe_flu - data_merged_np$fork_flu)
sd(data_merged_np$shoe_ela - data_merged_np$fork_ela)

# playful
mean(data_merged_p$shoe_flex - data_merged_p$fork_flex)
mean(data_merged_p$shoe_ori - data_merged_p$fork_ori)
mean(data_merged_p$shoe_flu - data_merged_p$fork_flu)
mean(data_merged_p$shoe_ela - data_merged_p$fork_ela)

sd(data_merged_p$shoe_flex - data_merged_p$fork_flex)
sd(data_merged_p$shoe_ori - data_merged_p$fork_ori)
sd(data_merged_p$shoe_flu - data_merged_p$fork_flu)
sd(data_merged_p$shoe_ela - data_merged_p$fork_ela)
```

### Runco et al 1987

Runco et al. (1987) compared (a) the summation scores (flexibility, fluency, and originality added together), (b) common and uncommon (the latter ideas produced by less than 5% of the sample), (c) ratio scores (originality or flexibility divided by fluency), and (d) weighted fluency scores (where weights are determined based on rarity of ideas, with less frequent ideas given more weight). They concluded that the weighted system was preferable.

#### comparing ratio scores

```{r}
# originality
# non-playful
ratio_score_np_pre_o <-round(mean(data_merged_np$fork_ori), 2)/round(mean(data_merged_np$fork_flu), 2)
ratio_score_np_pre_o

ratio_score_np_post_o <-round(mean(data_merged_np$shoe_ori), 2)/round(mean(data_merged_np$shoe_flu), 2)
ratio_score_np_post_o

# playful
ratio_score_p_pre_o <-round(mean(data_merged_p$fork_ori), 2)/round(mean(data_merged_p$fork_flu), 2)
ratio_score_p_pre_o

ratio_score_p_post_o <-round(mean(data_merged_p$shoe_ori), 2)/round(mean(data_merged_p$shoe_flu), 2)
ratio_score_p_post_o

# flexibility
# non-playful
ratio_score_np_pre_f <-round(mean(data_merged_np$fork_flex), 2)/round(mean(data_merged_np$fork_flu), 2)
ratio_score_np_pre_f

ratio_score_np_post_f <-round(mean(data_merged_np$shoe_flex), 2)/round(mean(data_merged_np$shoe_flu), 2)
ratio_score_np_post_f

# playful
ratio_score_p_pre_f <-round(mean(data_merged_p$fork_flex), 2)/round(mean(data_merged_p$fork_flu), 2)
ratio_score_p_pre_f

ratio_score_p_post_f <-round(mean(data_merged_p$shoe_flex), 2)/round(mean(data_merged_p$shoe_flu), 2)
ratio_score_p_post_f
```

#### weighted fluency scores

https://www.tutorialspoint.com/how-to-calculate-weighted-mean-in-r

```{r}
# in general

weighted_mean_fork <- weighted.mean(data_merged$fork_ori,data_merged$fork_flu)
weighted_mean_fork

weighted_mean_shoe <- weighted.mean(data_merged$shoe_ori,data_merged$shoe_flu)
weighted_mean_shoe

# non-playful

weighted_mean_np_fork <- weighted.mean(data_merged_np$fork_ori,data_merged_np$fork_flu)
weighted_mean_np_fork

weighted_mean_np_shoe <- weighted.mean(data_merged_np$shoe_ori,data_merged_np$shoe_flu)
weighted_mean_np_shoe

# playful

weighted_mean_p_fork <- weighted.mean(data_merged_p$fork_ori,data_merged_p$fork_flu)
weighted_mean_p_fork

weighted_mean_p_shoe <- weighted.mean(data_merged_p$shoe_ori,data_merged_p$shoe_flu)
weighted_mean_p_shoe
```

### mean scores (pre vs post) with t-test

```{r}
# flexibility
t.test(data_merged$fork_flex, data_merged$shoe_flex, paired = TRUE)

# originality
t.test(data_merged$fork_ori, data_merged$shoe_ori, paired = TRUE)

# fluency
t.test(data_merged$fork_flu, data_merged$shoe_flu, paired = TRUE)

# elaboration
t.test(data_merged$fork_ela, data_merged$shoe_ela, paired = TRUE)

# it is only the elaboration where the mean is higher in post-intervention
```

### t-test (each objective - pre vs post)

A paired-samples t-test was used to compare the creativity of the responses before and after the stereotype threat intervention for originality, flexibility, fluency, and elaboration. An alpha level of 5% was used to determine the level of significance in differences in the four creativity dimensions. Statistical analysis was performed using R [29].

```{r}
# non-playful

# flexibility
t.test(data_merged_np$fork_flex, data_merged_np$shoe_flex, paired = TRUE)

# originality
t.test(data_merged_np$fork_ori, data_merged_np$shoe_ori, paired = TRUE)

# fluency
t.test(data_merged_np$fork_flu, data_merged_np$shoe_flu, paired = TRUE)

# elaboration
t.test(data_merged_np$fork_ela, data_merged_np$shoe_ela, paired = TRUE)

# playful

# flexibility
t.test(data_merged_p$fork_flex, data_merged_p$shoe_flex, paired = TRUE)

# originality
t.test(data_merged_p$fork_ori, data_merged_p$shoe_ori, paired = TRUE)

# fluency
t.test(data_merged_p$fork_flu, data_merged_p$shoe_flu, paired = TRUE)

# elaboration
t.test(data_merged_p$fork_ela, data_merged_p$shoe_ela, paired = TRUE)
```
# t-test (between conditions)

```{r}
# flexibility
t.test(data_merged_p$shoe_flex, data_merged_np$shoe_flex, alternative = "two.sided", var.equal = FALSE)

# originality
t.test(data_merged_p$shoe_ori, data_merged_np$shoe_ori, alternative = "two.sided", var.equal = FALSE)

# fluency
t.test(data_merged_p$shoe_flu, data_merged_np$shoe_flu, alternative = "two.sided", var.equal = FALSE)

# elaboration
t.test(data_merged_p$shoe_ela, data_merged_np$shoe_ela, alternative = "two.sided", var.equal = FALSE)

# overall difference among conditions
t.test(data_merged_p$diff_all, data_merged_np$diff_all, alternative = "two.sided", var.equal = FALSE)
mean(data_merged_p$diff_all)
mean(data_merged_np$diff_all)
sd(data_merged_p$diff_all)
sd(data_merged_np$diff_all)
```

### comparing self-perceived creativity between-groups

```{r}
t.test(data_merged_p$creativity, data_merged_np$creativity, alternative = "two.sided", var.equal = FALSE)

mean(data_merged_p$creativity)
mean(data_merged_np$creativity)
```

### calculating stuff for the methods

```{r}
# task success rate
success_np <- c()
success_pl <- c()

for (i in 1:length(conditions$condition)) {
    if (conditions$condition[i] == 0) {
    success_np <- append(success_np, conditions$task_success[i])
    } else {
    success_pl <- append(success_pl, conditions$task_success[i])
    }
}

sd_np <- sd(success_np)
mean_np <- mean(success_np)
sd_np
mean_np

sd_pl <- sd(success_pl)
mean_pl <- mean(success_pl)
sd_pl
mean_pl

# age
sd_age <- sd(conditions$age)
mean_age <- mean(conditions$age)

sd_age
mean_age

# nationality
nat <- conditions %>% 
  group_by(nationality) %>% 
  summarise(total_count=n(),
            .groups = 'drop')
nat
```
### comparing copy people to non-copy people

```{r}
data_merged$all_same <- conditions$all_same
data_merged

# creating a dataset for each case
data_merged_diff <- data_merged %>% 
  filter(all_same == 0)

data_merged_same <- data_merged %>% 
  filter(all_same == 1)

# SIGNIFICANT
lm_model <- lm(shoe_all ~ all_same, data = data_merged)
summary(lm_model)

lm_model <- lm(diff_all ~ all_same, data = data_merged)
summary(lm_model)

plot(lm_model)

ggplot(data_merged, aes(x = as.factor(all_same), y = shoe_all)) + geom_boxplot() 

lm_model <- lm(shoe_all ~ condition, data = data_merged)
summary(lm_model)

lm_model <- lm(shoe_all ~ all_same + condition, data = data_merged)
summary(lm_model)

lm_model <- lm(shoe_all ~ all_same * condition, data = data_merged)
summary(lm_model)

lmer_model <- lmer(shoe_all ~ all_same +  condition + (1 | subject), data = data_merged)
anova(lmer_model)
summary(lmer_model)

glmer_model <- glm(condition ~ shoe_all, family=binomial, data=data_merged)
summary(glmer_model)
```
### barplots

```{r}
par(mfrow=c(2,2))

# flexibility
flex_plot_data_1 <- tibble(data_merged$condition, data_merged$fork_flex)
colnames(flex_plot_data_1) <- c("condition", "flexibility")
flex_plot_data_1$phase <- "pre-intervention"
flex_plot_data_1$phase_order <- 0
flex_plot_data_2 <- tibble(data_merged$condition, data_merged$shoe_flex)
colnames(flex_plot_data_2) <- c("condition", "flexibility")
flex_plot_data_2$phase <- "post-intervention"
flex_plot_data_2$phase_order <- 1
flex_plot_data <- rbind(flex_plot_data_1, flex_plot_data_2)

flex_plot <- ggplot(flex_plot_data, aes(fill=reorder(phase, +phase_order), y=flexibility, x=as.factor(condition))) + 
    geom_bar(position="dodge", stat="identity") + 
    labs(title = "",
       x = "",
       y = "") +
    scale_x_discrete(labels=c("0" = "Non-playful", "1" = "Playful")) +
    scale_fill_discrete(name = "Phase") +
    scale_fill_brewer(palette = "RdYIGn") +
    theme(panel.background = element_blank()) + 
    guides(fill=guide_legend(title="Phase"))

# originality
ori_plot_data_1 <- tibble(data_merged$condition, data_merged$fork_ori)
colnames(ori_plot_data_1) <- c("condition", "originality")
ori_plot_data_1$phase <- "pre-intervention"
ori_plot_data_1$phase_order <- 0
ori_plot_data_2 <- tibble(data_merged$condition, data_merged$shoe_ori)
colnames(ori_plot_data_2) <- c("condition", "originality")
ori_plot_data_2$phase <- "post-intervention"
ori_plot_data_2$phase_order <- 1
ori_plot_data <- rbind(ori_plot_data_1, ori_plot_data_2)

ori_plot <- ggplot(ori_plot_data, aes(fill=reorder(phase, +phase_order), y=originality, x=as.factor(condition))) + 
    geom_bar(position="dodge", stat="identity") + 
    labs(title = "",
       x = "",
       y = "") +
    scale_x_discrete(labels=c("0" = "Non-playful", "1" = "Playful")) +
    scale_fill_discrete(name = "Phase") +
    scale_fill_brewer(palette = "RdYIGn") +
    theme(panel.background = element_blank()) + 
    guides(fill=guide_legend(title="Phase"))

# fluency
flu_plot_data_1 <- tibble(data_merged$condition, data_merged$fork_flu)
colnames(flu_plot_data_1) <- c("condition", "fluency")
flu_plot_data_1$phase <- "pre-intervention"
flu_plot_data_1$phase_order <- 0
flu_plot_data_2 <- tibble(data_merged$condition, data_merged$shoe_flu)
colnames(flu_plot_data_2) <- c("condition", "fluency")
flu_plot_data_2$phase <- "post-intervention"
flu_plot_data_2$phase_order <- 1
flu_plot_data <- rbind(flu_plot_data_1, flu_plot_data_2)

flu_plot <- ggplot(flu_plot_data, aes(fill=reorder(phase, +phase_order), y=fluency, x=as.factor(condition))) + 
    geom_bar(position="dodge", stat="identity") + 
    labs(title = "",
       x = "",
       y = "") +
    scale_x_discrete(labels=c("0" = "Non-playful", "1" = "Playful")) +
    scale_fill_discrete(name = "Phase") +
    scale_fill_brewer(palette = "RdYIGn") +
    theme(panel.background = element_blank()) + 
    guides(fill=guide_legend(title="Phase"))

# elaboration
ela_plot_data_1 <- tibble(data_merged$condition, data_merged$fork_ela)
colnames(ela_plot_data_1) <- c("condition", "elaboration")
ela_plot_data_1$phase <- "pre-intervention"
ela_plot_data_1$phase_order <- 0
ela_plot_data_2 <- tibble(data_merged$condition, data_merged$shoe_ela)
colnames(ela_plot_data_2) <- c("condition", "elaboration")
ela_plot_data_2$phase <- "post-intervention"
ela_plot_data_2$phase_order <- 1
ela_plot_data <- rbind(ela_plot_data_1, ela_plot_data_2)

ela_plot <- ggplot(ela_plot_data, aes(fill=reorder(phase, +phase_order), y=elaboration, x=as.factor(condition))) + 
    geom_bar(position="dodge", stat="identity") + 
    labs(title = "",
       x = "",
       y = "") +
    scale_x_discrete(labels=c("0" = "Non-playful", "1" = "Playful")) +
    scale_fill_discrete(name = "Phase") +
    scale_fill_brewer(palette = "RdYIGn") +
    theme(panel.background = element_blank()) + 
    guides(fill=guide_legend(title="Phase"))

leg <- get_legend(ela_plot)

figure <- ggarrange(flex_plot, ori_plot, flu_plot, ela_plot,
                    labels = c("Flexibility", "Originality", "Fluency", "Elaboration"),
                    ncol = 2, nrow = 2, common.legend = TRUE, legend = "bottom", legend.grob = NULL)
figure
```

### boxplots

```{r}
# creating a df for each condition
diff_np <- data_merged_np$diff_all
diff_np <- cbind(diff_np, condition="non-playful")

diff_p <- data_merged_p$diff_all
diff_p <- cbind(diff_p, condition="playful")

diffs <- rbind(diff_np, diff_p)

# converting it to dataframe
diffs <- as.data.frame(diffs)

# renaming the column
names(diffs)[names(diffs) == "diff_np"] <- "difference"

boxplot_diff_np <- ggplot(diffs, aes(x=factor(condition), y=as.numeric(difference), fill=factor(condition))) + 
  geom_boxplot() +
  labs(y = "Difference between trials", x = "Condition") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
  panel.background = element_blank(), axis.line = element_line(colour = "black")) +
  scale_fill_brewer(palette = "RdYIGn")

boxplot_diff_np
```

### log likelihood calculation

TODO: fix colours

```{r}
# FORK
# corpus
fork_docs <- VectorSource(data$fork_use_nlp)
fork_corpus <- Corpus(fork_docs)

# set locale
Sys.setlocale("LC_COLLATE","C")

# set up document term matrix
fork_dtm <- DocumentTermMatrix(fork_corpus, control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3))

# removing way too common words and rare words
term_tfidf <- tapply(brick_dtm$v/row_sums(brick_dtm)[brick_dtm$i], brick_dtm$j, mean) * log2(nDocs(brick_dtm)/col_sums(brick_dtm > 0))

fork_dtm <- fork_dtm[ ,term_tfidf > 0.1]
fork_dtm <- fork_dtm[row_sums(fork_dtm) > 0,]

# setting seed
set.seed(4570)

k = seq.int(from = 5, to = 60, by = 5) # arbitrary

loglike <- vector("numeric", length = length(k))
for (i in 1:length(k)){
  x <- LDA(brick_dtm, k = k[i], method = "Gibbs", control = list(burnin = 1000, thin = 100, iter = 1000))
  loglike[i] <- x@loglikelihood
}
plotloglike <- data.frame("logLik" = loglike, "topics" = k)

# finding the best value
good_topic_nr_ind <- which(plotloglike$logLik == max(plotloglike$logLik))
good_topic_nr <- plotloglike$topics[which(plotloglike$logLik == max(plotloglike$logLik))]

require(ggplot2)
ggplot(plotloglike, aes(x = topics, y = logLik)) +
  geom_line(colour = "darkolivegreen4") +
  scale_x_continuous(breaks = seq(5, 60, by = 5)) +
  labs(x = "Number of Topics", y = "Log Likelihood") +
  theme_bw() +
  theme(axis.line = element_line(color='black'),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.border = element_blank()) +
  geom_rect(aes(xmin=good_topic_nr-1, xmax=good_topic_nr+1, ymin=-Inf, ymax=Inf), alpha=0.05, fill='gray78') +
  geom_vline(xintercept = good_topic_nr) +
  ggtitle("Log Likelihood on Number of Topics (Pre-intervention)") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))

fork_topic <- LDA(fork_dtm, k = 15, method = "Gibbs", control = list(burnin = 1000, thin = 100, iter = 1000))

#mean entropy over topics (higher values = even spread)
mean(apply(posterior(fork_topic)$topics, 1, function(z) - sum(z*log(z))))

Topic_list <- topics(fork_topic, 1) # for each document, it returns the most likely topic
```

TODO: remove thsi section

```{r}
data_nlp <- read.csv('data_nlp.csv', sep=',')
# calculating the mean per subject
# (the values were calculated calculated manually in the datasheet)

# fork
data_ela_fork <- aggregate(data = data_nlp,
                          as.numeric(fork_ela_nlp) ~ subject,
                          na.action = na.omit,
                          mean)

colnames(data_ela_fork) <- c("subject", "fork_word_count")

# rounding numbers
data_ela_fork$fork_word_count <- round(data_ela_fork$fork_word_count, digits=2)

data_ela_fork

# shoe
data_ela_shoe <- aggregate(data = data_nlp,
                          as.numeric(shoe_ela_nlp) ~ subject,
                          na.action = na.omit,
                          mean)

colnames(data_ela_shoe) <- c("subject", "shoe_word_count")

# rounding numbers
data_ela_shoe$shoe_word_count <- round(data_ela_shoe$shoe_word_count, digits=2)

data_ela_shoe

data_ela <- tibble(data_ela_fork, data_ela_shoe$shoe_word_count, conditions$condition)
colnames(data_ela) <- c("subject", "fork_word_count", "shoe_word_count", "condition")

# adding a column with the difference
data_ela$diff <- data_ela$shoe_word_count - data_ela$fork_word_count

data_ela

data_ela %>%
  group_by(condition) %>%
  summarise_at(vars(shoe_word_count), list(name = mean))

sapply(data_ela[,-1], function(x) sd(df1[!is.na(x), 1]))
```

